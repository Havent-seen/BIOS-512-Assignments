{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d684172-4b94-42cf-bda2-e11952420d86",
   "metadata": {},
   "source": [
    "# Homework 10\n",
    "#### Course Notes\n",
    "**Language Models:** https://github.com/rjenki/BIOS512/tree/main/lecture17  \n",
    "**Unix:** https://github.com/rjenki/BIOS512/tree/main/lecture18  \n",
    "**Docker:** https://github.com/rjenki/BIOS512/tree/main/lecture19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d839a5ba-62f4-4699-baea-018afda70786",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "#### Make a language model that uses ngrams and allows the user to specify start words, but uses a random start if one is not specified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef37d3a-a6ad-42ae-9e16-7d7338c9ce49",
   "metadata": {},
   "source": [
    "#### a) Make a function to tokenize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f896e2cf-c607-4518-90c2-3575da84b3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize <- function(text) {\n",
    "  text <- tolower(text)\n",
    "  \n",
    "  text <- gsub(\"[[:punct:]]\", \" \", text)\n",
    "  \n",
    "  tokens <- unlist(strsplit(text, \"\\\\s+\"))\n",
    "  \n",
    "  tokens <- tokens[tokens != \"\"]\n",
    "  \n",
    "  return(tokens)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86145513-294b-4894-a02c-8ae60e2c616e",
   "metadata": {},
   "source": [
    "#### b) Make a function generate keys for ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfc4b5c9-2fb2-4a50-a517-0c67af9fe91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_ngram_keys <- function(tokens, n) {\n",
    "  if (length(tokens) < n) {\n",
    "    return(character(0))\n",
    "  }\n",
    "  \n",
    "  keys <- c()\n",
    "  for (i in 1:(length(tokens) - n + 1)) {\n",
    "    key <- paste(tokens[i:(i + n - 2)], collapse = \" \")\n",
    "    keys <- c(keys, key)\n",
    "  }\n",
    "  return(keys)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52988c2c-b230-467f-b519-72bc85b93b43",
   "metadata": {},
   "source": [
    "#### c) Make a function to build an ngram table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51b2edf1-1f66-492d-a07a-9da445cc285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_ngram_table <- function(tokens, n) {\n",
    "  if (length(tokens) < n) {\n",
    "    return(data.frame(\n",
    "      key = character(),\n",
    "      next_word = character(),\n",
    "      count = integer()\n",
    "    ))\n",
    "  }\n",
    "\n",
    "  keys <- generate_ngram_keys(tokens, n)\n",
    "  next_words <- tokens[n:length(tokens)]\n",
    "  tab <- as.data.frame(\n",
    "    table(key = keys, next_word = next_words),\n",
    "    stringsAsFactors = FALSE\n",
    "  )\n",
    "  names(tab) <- c(\"key\", \"next_word\", \"count\")\n",
    "  \n",
    "  return(tab)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca6db37-abce-4705-9784-e1b898174f00",
   "metadata": {},
   "source": [
    "#### d) Function to digest the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41bc0343-7499-4f27-9586-1ceb66e02ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "digest_text <- function(text, n) {\n",
    "  if (length(text) > 1) {\n",
    "    text <- paste(text, collapse = \" \")\n",
    "  }\n",
    "  tokens <- tokenize(text)\n",
    "  ngram_tab <- build_ngram_table(tokens, n)\n",
    "  return(ngram_tab)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fff313-0f13-479b-94df-7588c19fdd3d",
   "metadata": {},
   "source": [
    "#### e) Function to digest the url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79200fb9-934c-4c35-bb54-f6b57ee91642",
   "metadata": {},
   "outputs": [],
   "source": [
    "digest_url <- function(url, n) {\n",
    "  lines <- readLines(url, warn = FALSE)\n",
    "  text <- paste(lines, collapse = \" \")\n",
    "  ngram_tab <- digest_text(text, n)\n",
    "  return(ngram_tab)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa4e73-ee6f-4569-9a54-9d7f7eb3f80a",
   "metadata": {},
   "source": [
    "#### f) Function that gives random start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cac07cb-b3c6-4cfd-ae57-a64590185985",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_start <- function(ngram_table) {\n",
    "  if (nrow(ngram_table) == 0) {\n",
    "    stop(\"The ngram table is empty. Nothing to sample from.\")\n",
    "  }\n",
    "  start <- sample(unique(ngram_table$key), 1)\n",
    "  \n",
    "  return(start)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e998fb24-f2d6-41bc-a751-1f6accd3411f",
   "metadata": {},
   "source": [
    "#### g) Function to predict the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70fd6542-bca0-4002-8757-ac9a8ab456f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_next_word <- function(current_key, ngram_table) {\n",
    "  rows <- ngram_table[ngram_table$key == current_key, ]\n",
    "  if (nrow(rows) == 0) {\n",
    "    return(NA)\n",
    "  }\n",
    "  probs <- rows$count / sum(rows$count)\n",
    "  next_word <- sample(rows$next_word, size = 1, prob = probs)\n",
    "  return(next_word)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347f4002-4932-42c4-a4af-8689293a5857",
   "metadata": {},
   "source": [
    "#### h) Function that puts everything together. Specify that if the user does not give a start word, then the random start will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc9f2ed2-1616-4101-bef6-572c73dd4dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text <- function(ngram_table, n, length = 20, start = NULL) {\n",
    "  if (is.null(start)) {\n",
    "    start <- random_start(ngram_table)\n",
    "  }\n",
    "  current_tokens <- unlist(strsplit(start, \" \"))\n",
    "\n",
    "  output <- current_tokens\n",
    "  for (i in seq_len(length)) {\n",
    "    if (length(current_tokens) < (n - 1)) {\n",
    "      break\n",
    "    }\n",
    "    \n",
    "    key <- paste(\n",
    "      current_tokens[(length(current_tokens) - n + 2):length(current_tokens)],\n",
    "      collapse = \" \"\n",
    "    )\n",
    "      \n",
    "    next_word <- predict_next_word(key, ngram_table)\n",
    "    if (is.na(next_word)) {\n",
    "      break\n",
    "    }\n",
    "    output <- c(output, next_word)\n",
    "    current_tokens <- c(current_tokens, next_word)\n",
    "  }\n",
    "  return(paste(output, collapse = \" \"))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b742c67-907c-4bc7-8df1-c84fa65a7554",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "#### For this question, set `seed=2025`.\n",
    "#### a) Test your model using a text file of [Grimm's Fairy Tails](https://www.gutenberg.org/cache/epub/2591/pg2591.txt)\n",
    "#### i) Using n=3, with the start word(s) \"the king\", with length=15. \n",
    "#### ii) Using n=3, with no start word, with length=15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d28a0dc-f369-4507-9845-3ed01a984314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'the king saw him lying there on the moist ground and set it before his wedding the'"
      ],
      "text/latex": [
       "'the king saw him lying there on the moist ground and set it before his wedding the'"
      ],
      "text/markdown": [
       "'the king saw him lying there on the moist ground and set it before his wedding the'"
      ],
      "text/plain": [
       "[1] \"the king saw him lying there on the moist ground and set it before his wedding the\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'of marble and no one answered at last i went alone through a thick wood at the'"
      ],
      "text/latex": [
       "'of marble and no one answered at last i went alone through a thick wood at the'"
      ],
      "text/markdown": [
       "'of marble and no one answered at last i went alone through a thick wood at the'"
      ],
      "text/plain": [
       "[1] \"of marble and no one answered at last i went alone through a thick wood at the\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(2025)\n",
    "grimm_trigrams <- digest_url(\"https://www.gutenberg.org/cache/epub/2591/pg2591.txt\", n = 3)\n",
    "\n",
    "text_the_king <- generate_text(grimm_trigrams, n = 3, length = 15, start = \"the king\")\n",
    "\n",
    "text_random_start <- generate_text(grimm_trigrams, n = 3, length = 15)\n",
    "\n",
    "text_the_king\n",
    "text_random_start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e04b167-7f2c-4e0f-88e7-86ba5e8d74cc",
   "metadata": {},
   "source": [
    "#### b) Test your model using a text file of [Ancient Armour and Weapons in Europe](https://www.gutenberg.org/cache/epub/46342/pg46342.txt)\n",
    "#### i) Using n=3, with the start word(s) \"the king\", with length=15. \n",
    "#### ii) Using n=3, with no start word, with length=15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22d3c6dd-09c7-4e63-a748-da9493fc1668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'the king standing firm and being placed from the cemetery at linton heath and is figured on'"
      ],
      "text/latex": [
       "'the king standing firm and being placed from the cemetery at linton heath and is figured on'"
      ],
      "text/markdown": [
       "'the king standing firm and being placed from the cemetery at linton heath and is figured on'"
      ],
      "text/plain": [
       "[1] \"the king standing firm and being placed from the cemetery at linton heath and is figured on\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'ghent figured in full in the provision of an equipped archer to attend in person formed his'"
      ],
      "text/latex": [
       "'ghent figured in full in the provision of an equipped archer to attend in person formed his'"
      ],
      "text/markdown": [
       "'ghent figured in full in the provision of an equipped archer to attend in person formed his'"
      ],
      "text/plain": [
       "[1] \"ghent figured in full in the provision of an equipped archer to attend in person formed his\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(2025)\n",
    "\n",
    "armour_trigrams <- digest_url(\n",
    "  \"https://www.gutenberg.org/cache/epub/46342/pg46342.txt\",\n",
    "  n = 3\n",
    ")\n",
    "\n",
    "text_the_king_armour <- generate_text(\n",
    "  armour_trigrams,\n",
    "  n = 3,\n",
    "  length = 15,\n",
    "  start = \"the king\"\n",
    ")\n",
    "\n",
    "text_random_armour <- generate_text(\n",
    "  armour_trigrams,\n",
    "  n = 3,\n",
    "  length = 15\n",
    ")\n",
    "\n",
    "text_the_king_armour\n",
    "text_random_armour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fb37ad-8e7c-4e62-afc0-ba46d46401fc",
   "metadata": {},
   "source": [
    "#### c) Explain in 1-2 sentences the difference in content generated from each source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bd87ab-0b37-4794-b915-71e630dbb8d9",
   "metadata": {},
   "source": [
    "The text generated from Grimm’s Fairy Tales sounds more like a story, with characters, actions, and narrative elements (“the king… lying there… wedding”). In contrast, the text from Ancient Armour and Weapons in Europe is much more technical and descriptive, referring to armor, weapons, figures, and historical terms, so the generated sentences feel more like historical documentation than storytelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacb4eed-9a47-434c-ae95-028ade53cb04",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "#### a) What is a language learning model? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48849ab3-0cd0-4cbf-832b-7e4219378102",
   "metadata": {},
   "source": [
    "A language model is basically a program that tries to understand patterns in text so it can predict what words are likely to come next. You give it a bunch of text, and it “learns” things like which words tend to appear together, how sentences usually flow, and what structure the language has. Once it’s trained, you can ask it to generate new text that follows those same patterns. In short, it’s like teaching a computer to “guess the next word” over and over until it can write sentences that sound like they came from the original source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdb16d9-0615-4e21-b52e-946f273e2987",
   "metadata": {},
   "source": [
    "#### b) Imagine the internet goes down and you can't run to your favorite language model for help. How do you run one locally?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b371d75a-9100-42b2-8736-3d5279e2eb30",
   "metadata": {},
   "source": [
    "If I want to run a language model locally, the main idea is just to keep everything on my own machine instead of connecting to an online service. That usually means downloading whatever text I want to train on, building the n-gram table myself (like we did in this homework), and then generating text directly in R or Python. With the n-gram approach, everything is self-contained, no internet needed. For larger modern models you’d download the model files beforehand, install the right libraries, and run them offline, but the concept is the same: once you have the data and the code on your computer, you don’t have to rely on any external language model server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85a743b-f814-4a53-96e6-8bccb3d34ab8",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "#### Explain what the following vocab words mean in the context of typing `mkdir project` into the command line. If the term doesn't apply to this command, give the definition and/or an example.\n",
    "| Term | Meaning |  \n",
    "|------|---------|\n",
    "| **Shell** | The shell reads my command (mkdir project) and runs it. |\n",
    "| **Terminal emulator** | The terminal emulator is just the window where I type the command. |\n",
    "| **Process** | A process is a running program; mkdir becomes a short-lived process when executed. |\n",
    "| **Signal** | A signal is a way to interrupt or control a process (e.g., Ctrl+C), but it doesn’t really apply to mkdir. |\n",
    "| **Standard input** | Standard input is data sent into a program; mkdir doesn’t read any input from stdin. |\n",
    "| **Standard output** | Standard output is where a program prints messages; mkdir usually prints nothing unless there’s an error. |\n",
    "| **Command line argument** | A command line argument is extra information after the command; in mkdir project, project is the argument. |\n",
    "| **The environment** | The environment is the set of variables the shell gives to programs (like PATH); mkdir uses it to locate the command. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e41204-40d1-460b-9abf-d8890561033e",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "#### Consider the following command `find . -iname \"*.R\" | xargs grep read_csv`.\n",
    "#### a) What are the programs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4f1ae5-ef21-4c28-92a0-f8c6826f586a",
   "metadata": {},
   "source": [
    "find\n",
    "\n",
    "xargs\n",
    "\n",
    "grep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b354431-13a5-4cdb-9f0d-a8aeb383ac7c",
   "metadata": {},
   "source": [
    "#### b) Explain what this command is doing, part by part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f4d0ae-b144-4d07-b84e-59ba8d229de2",
   "metadata": {},
   "source": [
    "find . -iname \"*.R\"\n",
    "→ The find program searches in the current directory (.) for files whose names end in .R, ignoring case.\n",
    "\n",
    "|\n",
    "→ The pipe takes the list of matching files and sends them to the next program.\n",
    "\n",
    "xargs grep read_csv\n",
    "→ xargs takes the file names from find and passes them as arguments to grep.\n",
    "→ grep read_csv searches inside those R files for the text read_csv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5657bf40-ac2c-47f2-baf9-9a137730a3d3",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "#### Install Docker on your machine. See [here](https://github.com/rjenki/BIOS512/blob/main/lecture18/docker_install.md) for instructions. \n",
    "#### a) Show the response when you run `docker run hello-world`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd9e3b7-47bf-4c3f-8483-21240e2e88c3",
   "metadata": {},
   "source": [
    "Hello from Docker!\n",
    "This message shows that your installation appears to be working correctly.\n",
    "\n",
    "To generate this message, Docker took the following steps:\n",
    " 1. The Docker client contacted the Docker daemon.\n",
    " 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n",
    "    (amd64)\n",
    " 3. The Docker daemon created a new container from that image which runs the\n",
    "    executable that produces the output you are currently reading.\n",
    " 4. The Docker daemon streamed that output to the Docker client, which sent it\n",
    "    to your terminal.\n",
    "\n",
    "To try something more ambitious, you can run an Ubuntu container with:\n",
    " $ docker run -it ubuntu bash\n",
    "\n",
    "Share images, automate workflows, and more with a free Docker ID:\n",
    " https://hub.docker.com/\n",
    "\n",
    "For more examples and ideas, visit:\n",
    " https://docs.docker.com/get-started/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14df42d-3e22-4543-b827-6558d611780c",
   "metadata": {},
   "source": [
    "#### b) Access Rstudio through a Docker container. Set your password and make sure your files show up on the Rstudio server. Type the command and the output you get below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7074f5-d84b-4f31-a6f9-0a06747e9954",
   "metadata": {},
   "source": [
    "C:\\Users\\15633\\Desktop\\BIOS_512\\assignment 10>docker run -d -p 8787:8787 -e PASSWORD=199831698Frankff -v \"%cd%\":/home/rstudio rocker/rstudio\n",
    "Unable to find image 'rocker/rstudio:latest' locally\n",
    "latest: Pulling from rocker/rstudio\n",
    "3665120d345d: Pull complete\n",
    "5d246ec925db: Pull complete\n",
    "3c7cdccc4be7: Pull complete\n",
    "890065c4c99d: Pull complete\n",
    "08e74fd5985d: Pull complete\n",
    "999e4b8f7ed8: Pull complete\n",
    "e4b9e87bb831: Pull complete\n",
    "2c9ba66d5dbe: Pull complete\n",
    "62f215ca34c6: Pull complete\n",
    "d923cf803a12: Pull complete\n",
    "4b3ffd8ccb52: Pull complete\n",
    "2a63ed8b2250: Pull complete\n",
    "9c1a4a0706b7: Pull complete\n",
    "b71e78fefbbb: Pull complete\n",
    "39038e16d1ba: Pull complete\n",
    "971ba7cf0d8a: Pull complete\n",
    "191985778909: Pull complete\n",
    "664fb1818bbb: Pull complete\n",
    "Digest: sha256:9f85211a666fb426081a6f5a01f9f9f51655262258419fa21e0ce38a5afc78d8\n",
    "Status: Downloaded newer image for rocker/rstudio:latest\n",
    "ca9bb4739fc47f6848d5e746076547c0cae5d6383ac88bb8852e18ea086e18e8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e2f5cc-c8ab-4a89-8a27-74f2d80251ac",
   "metadata": {},
   "source": [
    "#### c) How do you log in to the RStudio server?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3d34a6-1de6-47fc-b986-25b8bfed55ef",
   "metadata": {},
   "source": [
    "To log in to the RStudio server, I open a browser and go to\n",
    "http://localhost:8787.\n",
    "The username is rstudio, and the password is whatever I set in the Docker command (in my case, 199831698Frankff)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5062d7-dd05-492a-9a1d-a5a6878d4ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
